\section{Multi-Provider-Service-Schema}

Konfigurationsdateien sind im aktuellen Prototyp die einzige Benutzerschnittstelle. Sie liefern vielfältige Angaben: 

\begin{enumerate}
	\item Cloud-Provider-Zugangsdaten und -Metainformationen
	\item Service-Topologie, -Definition und -Konfiguration
	\item Service-Level-Objective-Anpassungen
\end{enumerate}

Alle nutzeranpassbaren Dateien folgen der vereinfachten Auszeichnungssprache YAML \emph{(YAML Ain't Markup Language)} zur datenorientierten Speicherung mithilfe von (assoziativen) Listen und Einzelwerten. Im Gegensatz zu XML oder JSON sind die resultierenden Konfigurationsdateien für Menschen einfacher lesbar. Gleichzeitig existieren Interpreter für alle verbreiteten Sprachen und Systeme. Durch die zeilenbasierte Speicherung einzelner Werte sind YAML-Dateien außerdem über Git versionierbar. Entsprechend hat sich der YAML-Standard als Konfigurationssyntax im Cloud- und DevOps-Kontext durchgesetzt; genutzt wird er zum Beispiel auch von AWS-Angeboten, Docker Compose oder Travis-CI. Als offener, menschen- und maschinenlesbarer Standard erfüllt YAML das Versprechen von Portabilität.

\begin{listing}[ht]	
	\inputminted[]{yaml}{./src/provider.sample.yaml}
	\caption{Provider-Definition und Zugangsdaten. Der Broker liest alle eingetragenen Accounts automatisch ein und berücksichtigt sie bei der initialen Service-Bereitstellung sowie in Optimierungsläufen. Public-Clouds benötigen nur Zugangsdaten wie Benutzername und Passwort -- alle weiteren Informationen erfragt der Broker dynamisch zur Laufzeit vom Provider. In Private-Cloud-Umgebungen ist dies nicht immer möglich: Details zur Verfügbarkeit, geografische Lage und Kosten müssen manuell eingepflegt oder vom Monitoring festgestellt werden.}
	\label{listing:provider}
\end{listing}

Ausschnitt \ref{listing:provider} zeigt die Definition der Serviceprovider: Public-Cloud-Angebote benötigen nur Angaben zur Authentifizierung, alle weiteren Informationen ruft Libcloud dynamisch zur Laufzeit ab. Dies können zum Beispiel aktuelle Preisinformationen, verfügbare Ressourcen und Geostandorte sein. Für private Infrastrukturen müssen diese Daten möglicherweise manuell eingetragen werden -- es liegt an den IT-Verantwortlichen, die anfallenden Kosten einer OpenStack-Instanz zu berechnen. Verfügbarkeitsstatistiken können direkt hinterlegt oder vom Broker berechnet werden. Obligatorisch ist die Angabe eines technischen Providers wie \emph{docker} oder \emph{ecs} und einer eindeutigen ID.

Services und SLOs sollen providerübergreifend, abstrakt definiert werden. In \autoref{sec:service-definition} haben wir hierzu den Standard TOSCA ausgewählt: Durch zweckmäßige Basistypen und Vererbung lassen sich übliche Webanwendungstopologien mit Qualitätsvereinbarungen kombinieren. Auch hier können wir YAML einsetzen -- dazu implementieren wir eine Untermenge von TOSCA-Simple. Wir definieren Basistypen für Services und importierbare Qualitätsziele. Nutzer können anschließend eigene Wünsche einbringen: Sie ergänzen durch Importe oder überschreiben Werte und Variablen manuell. Ausschnitt \autoref{listing:hyrise-r} zeigt die Definition für einen Hyrise-R-Dispatcher. Alle verbliebenen Variablen füllt der Broker dynamisch während der Planumsetzung. Zum Beispiel legt er fest, welcher Service-Teil, auf welchem Provider, mit welchem Instanz-Typen, bereitgestellt wird. Technisch basieren die Variablen auf der Jinja2-Syntax. 

Ausgeführte Pläne speichert der Broker mit ausgefüllten Variablen, Anwendungskennung und einer UUID als YAML-Dokumentation. Denkbar wäre auch eine Sicherung des Brokerergebnisses als CAMP-Plan, dieser wäre kompatibel mit anderen offenen Cloud-Management-Systemen. Diese Kooperation könnte ein Thema für weitere Arbeiten sein. Der Broker vergibt den gestarteten Instanzen außerdem Metainformationen als Labels, der Betrieb ist also theoretisch zustandslos: Aus den Zugangsinformationen zur Cloud-Infrastruktur und den Informationen der gestarteten Instanzen lassen sich die Anwendungen jederzeit rekonstruieren. Die Basisdefinitionen der Services liefern weitere Instruktionen zur Zustandsprüfung auf Service-Ebene und Reaktionen im Fehlerfall. Die folgende Auflistung erläutert alle Bestandteile unserer providerübergreifenden Servicedefininition:


\begin{description}
	
	\item[Kommentare] sind innerhalb der Service-Definition nicht vorgesehen. Mit einer Ausnahme: Eine Einleitung zu Art und Verwendung der YAML-Datei ist erlaubt. In diesem Fall handelt es sich zum Beispiel um eine Jinja2-Vorlage, die unterhalb des Verzeichnisses \emph{services/} platziert werden sollte.
	
	\item[Metadaten] Jede Service-Definition enthält eine Präambel aus verschiedenen Metadaten. Der Broker benötigt als erste Information eine Schema-Version. Zum Zeitpunkt der Arbeit existiert nur die Definition 1.0, dies könnte sich in Zukunft natürlich ändern, und sollte semantisch versioniert werden. Darüber hinaus ist auch der Inhalt eines Service-Schemas versioniert und datiert. 
	
	Die Informationen zu Autor und Lizenz sind für den Broker nicht wichtig, sie ermöglichen aber die Veröffentlichung eines Schemas in einem Cloud-übergreifenden Service-Katalog. Unterstützend sind die Links zu Quellcode, Website und Logo der integrierten Anwendung zusammen mit einer Kurzbeschreibung.

	\item[Importe] Aus TOSCA stammt die Idee von Service-Komponenten und -Klassen sowie Policy-Templates, die über einfache YAML-Importe eingebunden werden. Vorteil ist die schnellere Entwicklungszeit eines konkreten Services, nach dem \emph{DRY}-Prinzip. Die Importfunktion ist außerdem unabhängig vom Broker: Sie entspricht dem YAML-Standard. Der Verwaltungsaufwand verlagert sich allerdings, sobald die Vorlagen Service-übergreifend definiert werden und von der TOSCA-Standarddefinition abweichen.
	
	\item[Topologie] YAML-Auschnitt \autoref{listing:hyrise-r} zeigt den Inhalt der Liste \emph{services}: Sie enthält alle Anwendungskomponenten, die jeweils einzeln gestartet werden müssen. Jede Komponente hat eine eindeutige ID, hierüber lassen sich Attribute referenzieren.
	
	Ein Hyrise-Cluster besteht immer aus genau einem Dispatcher und genau einem Master -- beide Services sind entsprechend als \emph{global} gekennzeichnet. Dagegen sind Replica-Knoten optional und können in beliebiger Anzahl ergänzt werden. \emph{depends\_on} definiert diese Abhängigkeiten untereinander: Der Master benötigt einen gestarteten Dispatcher zur Registrierung. Replicas erwarten zusätzlich einen Master, um den Datenbestand abzugleichen.	
	
	\item[Laufzeitumgebung] und Konfigurationsschnittstelle unterschieden sich je nach Provider. Für Docker definieren wir ein Container-Image zum Download aus dem zentralen Docker Hub. Sowohl lokale Installationen als auch Cloud-Container-Dienste beziehen ihre Images hieraus. OpenStack besitzt den eigenen Image-Dienst \emph{Glance} -- ist ein Image noch nicht vorhanden, laden wir es bei Bedarf hoch.
	
	Die Erstkonfiguration erfolgt über den Docker Entrypoint beziehungsweise cloud-init. In beiden Fällen sind die Konfigurationsvariablen zu IP-Adressen, Ports und IDs in \emph{Jinja2}-Syntax angelegt. Der Broker füllt sie dynamisch während der Planung.
	
	\item[Ressourcen] Jeder Service benötigt ein Minimum an zugeteilten Ressourcen; CPU-Leistung, Netzwerkbandbreite, RAM- und HDD-Größe. Für Spezialanwendungen kommen noch besondere Anforderungen wie GPU-Funktionen oder FPGA-Zugriff hinzu. Besonders im Fall von Docker können außerdem erweiterte Rechte notwendig sein, die ein Standardcontainer nicht erhält. 
	
	Ist der erhöhte Ressourcenbedarf einer Anwendung von vornherein bekannt, kann das Template entsprechend angepasst werden. Auch die Angabe eines Ressourcenmaximums ist sinnvoll: Kaum ein Service skaliert effizient mit unbegrenzt vielen CPU-Kernen.
	
	\item[Netzwerk] Zur Kommunikation mit anderen Services, Endbenutzern und der Cloud-Management-Plattform benötigen Anwendungen Netzwerkzugriff. Sinnvoll ist eine Trennung der verschiedenen Belange in unabhängigen Netzwerken. Durch die VPN-Integration vieler Infrastrukturanbieter lässt sich diese Anforderung umsetzen. 
	
	Vordefiniert sind die Netzwerke \emph{Frontend} und \emph{Backend}. Ersteres ist in der Standardeinstellung für Nutzer von außerhalb erreichbar, Letzteres dient der Administration und dem Datenaustausch zwischen den Services selbst.
		
	\item[Wartung] Im Gegensatz zu einem einfachen Broker kümmert sich die Cloud-Management-Plattform auch um den ordnungsgemäßen Weiterbetrieb einer Anwendung. Wir definieren daher eine rudimentäre Überprüfung der Funktionsfähigkeit: In einem bestimmten Intervall soll die CMP eine HTTP-Schnittstelle ansprechen. Kommt auch nach drei Versuchen keine gültige Antwort, gilt die Service-Instanz als verloren und wird neu gestartet. Über weitere SLOs lässt sich das Verhalten anpassen. Für Hyrise existiert kein Stopp-Kommando: Der Dispatcher erkennt abgeschaltete Knoten selbstständig.
	
\end{description}

\begin{listing}[ht]	
	\inputminted[firstline=15]{yaml}{./src/hyrise-r.sample.yaml}
	\caption{Providerübergreifende Servicevorlage. Der Ausschnitt zeigt die Definition des zentralen \emph{Hyrise-R-Dispatcher}-Dienstes. Nicht zu sehen sind Metadaten und die übrigen Anwendungsbestandteile. Parameter werden zur Laufzeit vom Broker eingesetzt.}
	\label{listing:hyrise-r}
\end{listing}

Am Beispiel von OpenStack, AWS und Hyrise-R haben wir die Definition von providerübergreifenden Services und Qualitätsvereinbarungen in einem offenen YAML-Standard beschrieben. Das nächste Kapitel zeigt eine konkrete Teststellung und versucht unsere Forschungshypothese zu validieren.