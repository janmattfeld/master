\chapter{Implementierung}

\section{Testumgebung: OpenStack, Docker \& Hyrise-R}

Hyrise is an in-memory research database developed by the
Enterprise Platform and Integration Concepts (EPIC) group at
Hasso Plattner Institute Potsdam 3 . It shares several features
with SAP HANA. These include a delta store, column-based
storage, dictionary encoding, several compression techniques
as well as an insert-only approach. Hyrise also features a
remarkable OLAP performance and optimizations for OLTP
tasks [1].

Following the scale-out paradigm, Hyrise-R extends the
base project to a cloud-based in-memory solution. Figure 2
shows the architecture consisting of a Hyrise master node,
several replicas and one dispatcher [2]. Within SSICLOPS,
privacy and security concerns as well as performance and
reliability of such setups will be evaluated [3].

http://hpi.de/plattner/projects/hyrise.html

\section{Multi-Cloud-Bibliotheken}

Ziel ist die Implementierung eines externen Broker-Services oder die Aufwertung einer verteilten Anwendung für den automatischen Betrieb in mehreren Clouds. Da unabhängige Cloud-Provider keine einheitlichen APIs anbieten, stellt dieses Kapitel verschiedene Bibliotheken vor, um möglichst viel der zusätzlichen Komplexität zu verbergen.

Ohne weitere Bibliotheken müsste für jede zu berücksichtigende Cloud das jeweilige SDK eingebunden werden. Auch Namensgebung, Architektur und Prozesse unterscheiden sich von Anbieter zu Anbieter. 

Durch den Einsatz einer Dritt-Bibliothek ergibt sich allerdings eine potentielle Schwachstelle. Falls diese fehlerhaft ist, oder gar nicht weiter entwickelt wird, gefährdet dies das ganze Projekt. Historie und Zukunftschancen spielen bei der Auswahl eine zentrale Rolle. Im Optimalfall abstrahiert die Bibliothek Änderungen der Provider-SDKs. Ob und wie groß die Arbeitserleichterung ausfällt, prüft der Praxisteil.

Im Folgenden untersuchen wir die Eignung der populärsten Bibliotheken. Wichtigste Komponente ist dabei das Computing-Modul. Wünschenswert wäre auch Container-Unterstützung, um Images anbieterunabhängig bereitzustellen. Gestartete Anwendungskomponenten erfordern für die erste Erreichbarkeit oft Zugriff auf die DNS-Einstellungen der Cloud. Optional ist die Unterstützung von Content Delivery Networks, Speicher- und Backup-Diensten.

% https://tex.stackexchange.com/questions/341592/hyphenating-text-inside-tabularx
\begin{table*}\centering
	\begin{minipage}{\textwidth}
	\caption{Übersicht freier Multi-Cloud-Bibliotheken. Mit $*$ gekennzeichnete Eigenschaften sind experimentell. Aufgeführt sind nur die populärsten Cloud-Provider, die Bibliotheken können darüber hinaus weitere unterstützen. Ob eine Bibliothek weitere Informationen, wie aktuelle Preisinformationen und den Standort des Rechenzentrums abrufen kann, zeigt die Spalte \emph{Cost\,/\,Geo}.}
	\ra{1.3}
	\begin{tabularx}{\textwidth}{>{\centering}XXXr} \toprule
		Projekt & Cloud-Provider & Cloud-Services & Cost\,/\,Geo\\ \midrule
		Apache Libcloud (Python)\footnotemark & OpenStack, AWS, Azure, GCP, Docker & Compute, Container, DNS, Load Balancer, Storage, Backup & $x$\,/\,$x$\\
		Apache jclouds (Java)\footnotemark & OpenStack, AWS, Azure$*$, GCP, Docker & Compute, Container, Load Balancer$*$, Storage & $x$\,/\,$x$\\
		PkgCloud (Node.js)\footnotemark & OpenStack, AWS, Azure & Compute, Load Balancer, Storage$*$, DNS$*$ & --\,/\,--\\
		Libretto (Go)\footnotemark & OpenStack, AWS, Azure, GCP & Compute & --\,/\,--\\
		Fog (Ruby)\footnotemark & OpenStack, AWS, GCP & Compute, DNS, Storage & $x*$\,/\,--\\
		\bottomrule
	\end{tabularx}
	\label{tab:bibliotheken}
	\vspace{150pt}
	\footnotetext[1]{\url{https://libcloud.apache.org/}}
	\footnotetext[2]{\url{https://jclouds.apache.org/}}
	\footnotetext[3]{\url{https://github.com/pkgcloud/pkgcloud/}}
	\footnotetext[4]{\url{https://github.com/apcera/libretto/}}
	\footnotetext[5]{\url{http://fog.io/}}
\end{minipage}  
\end{table*}

\autoref*{tab:bibliotheken} listet die untersuchten Bibliotheken mit unterstützten Cloud-Providern, Services und weiteren Features. Letzteres sind Zugriff auf Preisinformationen des Anbieters und Standortinformationen der Rechenzentren. Zusätzlich sollten die Projekte kontinuierlich weiterentwickelt werden, eine aktive Community besitzen und gut dokumentiert sein. Alle sind Open Source und unter einer freien Lizenz verfügbar.

\begin{description}
	
	\item[Apache jclouds] existiert schon seit 2009. Es unterstützt zumindest experimentell die wichtigsten Provider, aber nicht alle Services: DNS ist nicht vorhanden, Container-Unterstützung gibt es nur für Docker. Die Bibliothek ist gut getestet, dokumentiert, und mit zahlreichen Beispielen ausgestattet. Durch Java ist sie außerdem typsicher.

	\item[Apache Libcloud] vereint viele Vorteile: Es unterstützt neben OpenStack, als Referenz für Private-Cloud-Installationen, alle großen und kleinen Cloud-Provider mit allen Kernservices. Besonders interessant ist der Container-Support für Docker, Kubernetes, Amazon ECS und die Google Container Engine. Entsprechend gepackte Anwendungen könnten in einer Vielzahl von Clouds ohne weitere Änderungen ausgeführt werden.

	\item[Fog] integriert die wichtigsten Anbieter und Services. Die Community rund um Fog ist aktiv und die Bibliothek wird häufig eingesetzt. Besonders interessant sind die bereitgestellten Mocks, die Tests des neuen Services erleichtern sollen. Zumindest für OpenStack wird Metering unterstützt. Eine einheitliche Namensgebung der verschiedenen Cloud-Produkte existiert nicht.

	\item[Libretto] beschränkt sich ausdrücklich auf die Compute-Funktionalität mithilfe virtueller Maschinen. Das zugehörige Projekt ist aktiv, kommt aufgrund der fehlenden Funktionalität aber nicht infrage.

	\item[PkgCloud] ist die einzige bekannte Node.js-Bibliothek. Funktionsumfang und einheitliche Namensgebung der Cloud-Services sind überzeugend; leider wird die Bibliothek seit dem Verkauf des federführenden Unternehmens nicht mehr aktiv gepflegt. Bereits eingereichte Pull Requests werden nicht bearbeitet. Damit scheidet PkgCloud für das Projekt aus.

\end{description}

\noindent Libcloud fasst die verschiedenen Cloud-Angebote nicht nur in gemeinsamen Namensräumen zusammen, sondern normalisiert auch Leistungsklassen. Python erleichtert außerdem den Einstieg und fügt sich in viele Python-basierte Systemautomatisierungen ein. Diese Multi-Cloud-Bibliothek wird also im weiteren Verlauf der Arbeit erprobt.

%young open source project, python, integrates SDKs instead of REST API, minimal set of features, unstable, OpenStack + AWS

\section{OpenStack-Testbed}

Als Beispiel für eine Private Cloud -- als Teil unseres Multi-Cloud-Setups -- soll OpenStack dienen. Es ist das populärste Open Source-Projekt um eigene Infrastruktur als Service aufzubauen. 

%Einführung hardware resources, create software-defined networks, com-
%pute nodes and storages. Users can manage the framework
%via a graphical dashboard (see Figure 1), a REST API or
%command line tools. OpenStack development is supported by
%large corporations such as HPE, IBM, AMD, Intel, Canonical,
%Red Hat, SUSE and more.
%Today, the OpenStack project is the foundation of multiple
%large private cloud environments. These include business (e. g.
%Open Telekom Cloud) as well as non

Multi-Cloud-Setups: Möglichkeiten und Entscheidung dagegen: Multi-Cloud, keine Föderation. Einrichtung aufwändig, Erkenntnisgewinn vermutlich gerin. Föderation auch nicht sinnvoll: da Ausfallsicherheit geringer.\todo{}

Selbst ein minimales OpenStack-Testsetup ist durch die diversen Dienste komplex. Denkbar wäre also auch die Nutzung von externen OpenStack-Angeboten. In diesem Projekt gibt es hierfür grundsätzlich drei mögliche Bereitstellungsmodelle:

\begin{enumerate}
	\item Public Cloud
	\item Hosted Private Cloud
	\item Lokale (Test-)Installation (physikalisch oder virtuell)
\end{enumerate}

\noindent Eine Liste öffentlicher OpenStack-Angebote findet sich auf der Projekthomepage\footnote{\url{https://www.openstack.org/marketplace/hosted-private-clouds/}}. Dort werden auch weitere Informationen wie Funktionsumfang und Zertifizierungen aufgeführt. 

Interessant ist zum Beispiel das Angebot der Deutschen Telekom \emph{Open Telekom Cloud\footnote{\url{https://cloud.telekom.de/en/infrastructure/open-telekom-cloud/}}}: eine Public Cloud auf OpenStack-Basis -- in Deutschland -- mit vollem Funktionsumfang und API-Zugriff. International bietet Rackspace eine Hosted Private Cloud\footnote{\url{https://www.rackspace.com/openstack/}}. Beide eignen sich jedoch kaum um kleine Experimente zu starten, sondern richten sich vor allem preislich an größere Organisationen und Unternehmen.

Kostenlos ist das Public Cloud-Angebot TryStack\footnote{\url{http://trystack.org/}}. Sponsoren wie Cisco, \mbox{NetApp}, Dell und Red Hat finanzieren das Projekt. Die Registrierung erfolgt über die Aufnahme in eine Facebook-Gruppe, anschließend soll hierüber auch der Zugang zur kostenlosen OpenStack-Liberty-Instanz erfolgen. Während der gesamten Laufzeit dieser Arbeit war allerdings weder ein Login noch Kontakt zu den Organisatoren möglich.

Lokale OpenStack-Installationen sind aufwändig: für jeden Dienst muss ein eigener physikalischer Rechner bereitstehen. Dementsprechend verweist die offizielle Dokumentation direkt auf die Vielzahl von OpenStack-Distributionen\footnote{\url{https://www.openstack.org/marketplace/distros/}}. Diese bieten fast immer einen vereinfachten Setup-Prozess und oft die Option statt physikalischen Rechnern virtuelle Maschinen oder Container zu nutzen. Wie auch bei den Hosted-Angeboten sind hier nicht alle Dienste verfügbar. In allen Paketen fehlt Zun, der aktuelle Container-Service.

Speziell für lokale Test- und Entwicklungsumgebungen existiert DevStack\footnote{\url{https://docs.openstack.org/devstack/latest/}}. Das offizielle OpenStack-Projekt installiert automatisiert die wichtigsten OpenStack-Dienste auf einer einzigen Maschine. Ausdrücklich unterstützt werden dabei auch VMs und LXC-Container. Es soll daher als erstes erprobt werden.


\section{DevStack virtualisiert inkl. Container-Support}


Während der Installation nimmt DevStack tiefgreifende Veränderungen am Hostsystem vor. Es müsste also auf einem separaten Server installiert werden. Dieser Abschnitt beschreibt den Versuch einen virtualisierten, reproduzierbaren DevStack-Testaufbau zu erstellen. Außerdem soll Zun integriert werden. 


Ziel ist DevStack in einem Container auszuführen, genauso wie die darin gestarteten Compute-Nodes ebenso in einem nun verschachtelten Container bereitgestellt werden sollen. Die Gründe hierfür sind zusammengefasst:

\begin{enumerate}
	\item Keine oder minimale Änderungen am Hostsystem
	\item Reproduzierbarer Testaufbau
	\item Schneller und rückstandsloser Reset
	\item Zustände (Snapshots) speicherbar
	\item Schnelle Ausführung von Gastapplikationen
\end{enumerate}

\noindent Auch eine virtuelle Maschine löst die oben genannten Probleme. Theoretisch. Problematisch wird die Ausführungsgeschwindigkeit von Gastanwendungen in einem mit \emph{VirtualBox} virtualisierten OpenStack. Eine Lösung ist \emph{verschachteltes KVM}, das bereits in der Arbeit [1] erprobt wurde. Die Autoren Empfehlen Ihren Vorschlag bei bestehenden Erfahrungen mit \emph{libvirt}. Der damalige Versuchsaufbau stellt sich allerdings als instabil und nicht (mehr) reproduzierbar heraus. % Link zum Masterprojekt

% OpenStack-Ansible benötigt zu lang zur Installation.

LXD-Container könnten sich ebenfalls eignen. Im Gegensatz zu Docker führen sie mehrere Prozesse aus, erinnern also mehr an eine klassische virtuelle Maschine (ohne deren Overhead). Laut Entwickler Canonical fokussiert sich LXD speziell auf IaaS-Aufgaben\footnote{\url{https://www.ubuntu.com/containers/lxd}}. Ein LXD-DevStack-Setup birgt allerdings die gleichen Hürden\footnote{\url{https://docs.openstack.org/devstack/latest/guides/lxc.html}} wie ein Docker-Setup\footnote{\url{https://stgraber.org/2016/10/26/lxd-2-0-lxd-and-openstack-1112/}}. Beachtenswert ist noch das OpenStack-Projekt \emph{Kolla}, das jeden OpenStack-Dienst in einem eigenen Docker-Container installiert\footnote{\url{https://cloudbase.it/openstack-kolla-hyper-v/}}.

Um Container innerhalb von OpenStack auszuführen gibt es mehrere, teils konkurrierende Projekte. Alle lassen sich über Plugins in DevStack einbinden. Dies sind die wichtigsten\footnote{\url{https://de.slideshare.net/openstackindia/state-of-containers-in-openstack
}}:

\begin{description}
	\item[Zun] Eigenständige OpenStack-API zum Starten und Verwalten von diversen Container-Typen, inklusive Docker.
	\item[Nova Docker\footnote{\url{https://wiki.openstack.org/wiki/Docker}}] Im Gegensatz zu Zun erfolgt die Docker-Container-Verwaltung über die bekannte Nova-API. Das Projekt wurde eingestellt.
	\item[Nova LXD] Parallel zu Nova Docker erfolgt der Zugriff über die Nova-API. Das Projekt wird von Canonical aktiv vorangetrieben. Weiterer Teil ist die Automatisierung via \emph{Juju}.
	\item[Magnum] Eine Self-Service-Lösung zur Orchestrierung auf Basis von \emph{Heat}. Stellt automatisiert Container Orchestration Engines (COEs) wie Docker Swarm und Kubernetes bereit.
\end{description}

\noindent DevStack in Docker wurde bereits vor einiger Zeit umgesetzt\footnote{\url{https://github.com/ewindisch/dockenstack}}. Da das Projekt nicht mehr gepflegt wird und auf das ebenfalls beendete Nova Docker aufsetzt, erfolgt die Neuimplementierung mit folgenden Änderungen:

\begin{itemize}
	
	\item Ubuntu 16.04 LTS Basis-Image
	\item systemd\footnote{\url{https://docs.openstack.org/devstack/latest/systemd.html}}
	\item OpenStack Ocata and Pike
	\item libvirt/QEMU-Instanzen
	\item Zun statt Nova Docker
	\item Container-angepasste DevStack-Konfiguration
	\item Vollständige Netzwerkkonfiguration
	
\end{itemize}

\todo{Architektur-Diagramm}

\noindent Größte Hürde ist die Limitierung auf einen Prozess innerhalb eines Standard-Docker-Containers. Neuere DevStack-Versionen setzen auf systemd. Daher muss dies über die Umgebungsvariable \emph{ENV container docker} bekannt gemacht werden. Anschließend lässt sich systemd über zwei weitere Workarounds starten\footnote{\url{https://github.com/moby/moby/issues/27202}}\footnote{\url{https://github.com/moby/moby/issues/9212}}.

Docker Build bereitet das Image mit allen externen DevStack-Abhängigkeiten vor. Notwendige Dienste wie RabbitMQ und MySQL werden bereits im Voraus installiert. Das Container-Images führt beim Start nur noch die allerletzten Schritte des Setups aus. Ganz vorweg nehmen lässt sich das Setup nicht, weil während des Builds keine erweiterten Rechte vorliegen.

Nach erfolgreichem Start reicht das Kommando \emph{make run} um per Zun einen Cirros-Basis-Container zu starten. Der Stand der gesamten OpenStack-Installation lässt sich per \emph{docker commit} oder experimentell per Docker-Snapshots\footnote{\url{https://criu.org/Docker}} sichern.

Anpassbar sind im Skript OpenStack-Services und -Versionen, da DevStack direkt aus den Quellen installiert wird. So ändern sich allerdings selbst die Abhängigkeiten der als stabil gekennzeichneten Versionen. Das Prinzip Infrastruktur als Code geht hier nicht auf -- an den zuverlässig reproduzierbaren DevStack-Einsatz ist nicht zu denken. 

Als Proof-of-Concept ist die Integration von Docker, DevStack und Zun bisher einmalig. Der Code ist daher auf GitHub\footnote{\url{https://github.com/janmattfeld/DockStack}} veröffentlicht und zeigt einige Best Practices und Lessons Learned in Bezug auf die genannten Projekte.

Letztendlich greifen wir auf eine lokale Mirantis-OpenStack-Installation aus einem anderen Projekt zurück. Die Infrastruktur ist virtuell und wird durch \emph{Fuel} zuverlässig wieder aufgebaut. Die Zun-Container-Dienste sind nicht enthalten; dafür aber alle anderen Kernfunktionen und APIs.